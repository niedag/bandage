{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/niedag/GitHub-workspace/healthy-rag/.venv/bin/pip: /home/niedag/GitHub-workspace/HP-AI-hackathon-2025/.venv/bin/python: bad interpreter: No such file or directory\n",
      "/bin/bash: /home/niedag/GitHub-workspace/healthy-rag/.venv/bin/pip: /home/niedag/GitHub-workspace/HP-AI-hackathon-2025/.venv/bin/python: bad interpreter: No such file or directory\n",
      "/bin/bash: /home/niedag/GitHub-workspace/healthy-rag/.venv/bin/pip: /home/niedag/GitHub-workspace/HP-AI-hackathon-2025/.venv/bin/python: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets langchain chromadb bitsandbytes accelerate langchain_community\n",
    "!pip install git+https://github.com/xlang-ai/instructor-embedding.git\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niedag/GitHub-workspace/healthy-rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os, torch, logging, time, atexit\n",
    "import bitsandbytes as bnb\n",
    "from getpass import getpass\n",
    "from datasets import load_dataset\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, pipeline, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "if \"HF_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HF_TOKEN\"] = getpass(\"Enter your Hugging Face otoken: \")\n",
    "    print(\"HF_TOKEN is set:\", \"HF_TOKEN\" in os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 01:20:05,719 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test output: None\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_dtype=\"nf4\")\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "    return model,tokenizer\n",
    "\n",
    "model,tokenizer = load_model_and_tokenizer()\n",
    "\n",
    "def verify_model(model,tokenizer):\n",
    "    input_text = \"Testin 123 123 123\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "\n",
    "print(f\"Model test output: {verify_model(model,tokenizer)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 47750.99 examples/s]\n",
      "/tmp/ipykernel_12440/640461930.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_fn = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceEmbeddings\nmodel_kwar1gs\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'device': 'cuda:0'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m         collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[1;32m     27\u001b[0m             texts\u001b[38;5;241m=\u001b[39mchunks,\n\u001b[1;32m     28\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpubid\u001b[39m\u001b[38;5;124m\"\u001b[39m]}] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunks)\n\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     31\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_pubmed_qa_dataset(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43minit_chroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m ingest_data(collection, dataset)\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36minit_chroma\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minit_chroma\u001b[39m():\n\u001b[0;32m----> 7\u001b[0m     embedding_fn \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-mpnet0base-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwar1gs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Chroma(\n\u001b[1;32m     12\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_fn,\n\u001b[1;32m     13\u001b[0m         client_settings\u001b[38;5;241m=\u001b[39mSettings(anonymized_telemetry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub-workspace/healthy-rag/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:224\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     emit_warning()\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub-workspace/healthy-rag/.venv/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:69\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the sentence_transformer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     72\u001b[0m         since \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/GitHub-workspace/healthy-rag/.venv/lib/python3.10/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceEmbeddings\nmodel_kwar1gs\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'device': 'cuda:0'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "def load_pubmed_qa_dataset(split=\"train\", limit= None):\n",
    "    dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=split)\n",
    "    return dataset.select(range(min(limit,len(dataset)))) if limit else dataset\n",
    "\n",
    "\n",
    "def init_chroma():\n",
    "    embedding_fn = HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/all-mpnet0base-v2\",\n",
    "        model_kwar1gs = {\"device\" : DEVICE}\n",
    "    )\n",
    "    return Chroma(\n",
    "        embedding_function=embedding_fn,\n",
    "        client_settings=Settings(anonymized_telemetry=False)\n",
    "    )\n",
    "\n",
    "def ingest_data(collection, dataset):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size =1024, chunk_overlap=64)\n",
    "    for entry in tqdm(dataset, desc=\"Ingesting data\"):\n",
    "        document = (\n",
    "            f\"Question: {entry.get('question', '')}\\n\",\n",
    "            f\"Context: {entry.get('context', '')}\\n\",\n",
    "            f\"Abstract: {entry.get('abstract', '')}\\n\",\n",
    "            f\"Long Answer: {entry.get('long)answer', '')}\"\n",
    "        )\n",
    "        chunks = text_splitter.split_text(document)\n",
    "        collection.add_texts(\n",
    "            texts=chunks,\n",
    "            metadatas=[{\"source\": entry[\"pubid\"]}] * len(chunks)\n",
    "        )\n",
    "\n",
    "dataset = load_pubmed_qa_dataset(limit=1000)\n",
    "collection = init_chroma()\n",
    "ingest_data(collection, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3042633771.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    return RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=collection.as_retriever(search_kwargs=(\"k\":2)), return_source_documents=False, chain_type_kwargs={\"prompt\":prompt})\u001b[0m\n\u001b[0m                                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"You are an AI assistant specializing in medical literature. Answer accurately and concisely based on the given context.\"\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"[INST] <<SYS>> {system_prompt} <</SYS>> {prompt} [/INST]\".strip()\n",
    "\n",
    "\n",
    "template = generate_prompt(\"{context}\\nQuestion: {question}\", system_prompt = \"Use the context to answer the medical question.\")\n",
    "prompt = PromptTemplate(template = template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "text_pipeline = pipeline(\"text-generations\", model=model, tokenizer=tokenizer, max_new_tokens = 500, temperature=0.1, top_p=0.95, repetition_penalty=1.15, streamer=streamer)\n",
    "llm = HuggingFacePipeline(pipeline = text_pipeline)\n",
    "\n",
    "def create_qa_chain(collection):\n",
    "    return RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=collection.as_retriever(search_kwargs=(\"k\":2)), return_source_documents=False, chain_type_kwargs={\"prompt\":prompt})\n",
    "\n",
    "qa_chain = create_qa_chain(collection)\n",
    "print(\"RetrievalQA chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_query_loop(qa_chain):\n",
    "    while True:\n",
    "        query = input (\"Enter your medical question (or 'quit' to exit): \")\n",
    "        if query.lower() == 'quit':break\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = qa_chain(query)\n",
    "            print(f\"Query: {query}\\nAnswer: {result['reuslt']}\\nResponse time: {time.time() - start_time :.2f} seconds\")\n",
    "        except Exception as e: \n",
    "            logging.error(f\"Error processing query: {e}\")\n",
    "\n",
    "def graceful_shutdown():\n",
    "    logging.info(\"Shutting down gracefully...\")\n",
    "\n",
    "atexit.register(graceful_shutdown)\n",
    "interactive_query_loop(qa_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
